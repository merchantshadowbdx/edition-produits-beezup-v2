import streamlit as st
import tempfile
from datetime import datetime

from beezup.client import BeezUPClient
from beezup.extractor import *
from beezup.formatter import *
from beezup.builder import build_and_export_excel

st.set_page_config(page_title="Edition produits BeezUP V2", layout="wide", page_icon="üêù")

# ---------- SIDEBAR ---------- #
with st.sidebar:
    st.image("d√©cembre.png", width="content", caption="Joyeuses f√™tes")
    st.title("Param√®tres")
    api_key = st.text_input("*Cl√© API BeezUP*", type="password", key="api_key")
    store_name = st.text_input("*Nom de la boutique*", key="store_name")
    catalog_id = st.text_input("*Channel Catalog ID*", key="catalog_id")

    if st.button("\u21bb R√©initialiser l'application", key="reset_app"):
        api_key_val = st.session_state.get("api_key", "")
        catalog_id_val = st.session_state.get("catalog_id", "")
        store_name_val = st.session_state.get("store_name", "")
    
        # üîπ Incr√©menter les cl√©s dynamiques pour forcer les resets des text_area
        eans_key = st.session_state.get("eans_text_key", "eans_text_0")
        eans_idx = int(eans_key.split("_")[-1]) + 1
    
        attr_key = st.session_state.get("attr_text_key", "attr_text_0")
        attr_idx = int(attr_key.split("_")[-1]) + 1
    
        # üîπ R√©initialiser le session_state en conservant les infos importantes
        st.session_state.clear()
        st.session_state["api_key"] = api_key_val
        st.session_state["catalog_id"] = catalog_id_val
        st.session_state["store_name"] = store_name_val
        st.session_state["eans_text_key"] = f"eans_text_{eans_idx}"
        st.session_state["attr_text_key"] = f"attr_text_{attr_idx}"
    
        st.rerun()

# ---------- ONGLETS ---------- #
tab1, tab2, tab3 = st.tabs(["G√©n√©rer un template", "√âditer les produits", "Mapper les attributs"])

# Nettoyer les statuts
def clear_after(step):
    clear_list = [
        "client", "store_id", "channel_id", "product_df", "attribute_df", "merged_df",
        "eans_validated", "attrs_validated", "selected_df", "template_generated",
        "template_df", "dropdown_df", "datainfo_df"
    ]
    for s in clear_list:
        if s in st.session_state and clear_list.index(s) > clear_list.index(step):
            del st.session_state[s]

# ---------- TAB1 : GENERER UN TEMPLATE ---------- #
with tab1:
    st.title("G√©n√©ration du template produits")

    # --- √âtape 1 : Saisie EANs
    with st.container(border=True):
        st.subheader("\u2776 EANs √† traiter")

        # Initialise la cl√© dynamique si besoin (juste avant le text_area)
        if "eans_text_key" not in st.session_state:
            st.session_state["eans_text_key"] = "eans_text_0"

        eans_text = st.text_area(
            "*Collez vos EANs (un par ligne)*",
            key=st.session_state["eans_text_key"]
        )

        # Saisie des EANs
        eans = [ean.strip() for ean in eans_text.splitlines() if ean.strip()]
        valider_eans = st.button("\u2713 Valider les EANs", key="validate_eans")

        if valider_eans:
            if api_key and catalog_id and eans:
                with st.spinner("G√©n√©ration des listes d'attributs en cours..."):

                    # Cr√©ation du BeezUPClient et extraction des IDs
                    client = BeezUPClient(api_key)
                    store_id, channel_id = get_store_and_channel_ids(client, catalog_id)
                    st.session_state["client"] = client
                    st.session_state["store_id"] = store_id
                    st.session_state["channel_id"] = channel_id

                    # Extraction et cr√©ation du dataframe product_df
                    product_infos = extract_products(client, catalog_id, eans)
                    all_override_keys, all_attr_mapping_keys = set(), set()

                    for prod in product_infos:
                        all_override_keys.update(prod.get("overrides", {}).keys())
                        all_attr_mapping_keys.update(prod.get("attributeMappingValue", {}).keys())

                    override_columns = sorted(list(all_override_keys))
                    attr_mapping_columns = sorted(list(all_attr_mapping_keys))
                    rows = []

                    for prod in product_infos:
                        row = {
                            "Product Id": prod.get("productId"),
                            "Offer Code": prod.get("productSku"),
                            "Name": prod.get("productTitle"),
                            "Catalog Id": catalog_id
                        }

                        for col in override_columns:
                            value = prod.get("overrides", {}).get(col, {})
                            row[col] = value.get("override") if isinstance(value, dict) else ""

                        for col in attr_mapping_columns:
                            value = prod.get("attributeMappingValue", {}).get(col, {})
                            row[col] = f"{value.get('attributeMappingValue')} | {value.get('catalogValue')}" if isinstance(value, dict) else ""

                        rows.append(row)

                    product_df = pd.DataFrame(rows)

                    # Extraction des column_ids des attributs c√¥t√© catalogue
                    catalog_columns = client.get_catalog_columns(store_id).get("catalogColumns", [])
                    categ3_column_id = get_catalog_column_id(catalog_columns, "categ3Code")
                    ean_column_id = get_catalog_column_id(catalog_columns, "ean")
                    description_column_id = get_catalog_column_id(catalog_columns, "description")

                    column_ids = {
                        "Category Code": categ3_column_id,
                        "EAN": ean_column_id,
                        "Description": description_column_id
                    }

                    # Ajout pour extraction des images
                    # Images 1..6 (on n‚Äôajoute que celles r√©ellement trouv√©es)
                    for i in range(1, 7):
                        img_id = get_catalog_column_id(catalog_columns, f"imageUrl{i}")
                        if img_id:  # si la colonne image{i} existe dans ce catalogue
                            column_ids[f"Image {i}"] = img_id

                    # Extraction et cr√©ation du dataframe octopia_df
                    product_ids = product_df["Product Id"].tolist()
                    octopia_df = extract_octopia_product_fields(client, store_id, column_ids, product_ids)

                    # Fusion des dataframes product_df et octopia_df sur la colonne "Product Id"
                    product_df["Product Id"] = product_df["Product Id"].astype(str).str.strip()
                    octopia_df["Product Id"] = octopia_df["Product Id"].astype(str).str.strip()

                    # Ajout pour extraction des images
                    # Colonnes fixes demand√©es depuis column_ids (en conservant l‚Äôordre ‚ÄúCategory/EAN/Description‚Äù devant)
                    base_fixed = ["Category Code", "EAN", "Description"]
                    image_fixed = [f"Image {i}" for i in range(1, 7) if f"Image {i}" in octopia_df.columns]
                    octo_cols = ["Product Id"] + base_fixed + image_fixed

                    merged_df = pd.merge(
                        product_df,
                        octopia_df[octo_cols],
                        # octopia_df[["Product Id", "Category Code", "EAN", "Description"]],
                        on="Product Id",
                        how="left"
                    )

                    # R√©organisation : colonnes fixes + overrides + mappings
                    cols_order = (
                            ["Category Code", "Product Id", "Offer Code", "EAN", "Name", "Description", "Catalog Id"] +
                            image_fixed +  # on ins√®re ici les images fixes trouv√©es
                            override_columns +
                            attr_mapping_columns
                    )

                    # # R√©organisation des colonnes
                    # cols_order = [
                    #     "Category Code", "Product Id", "Offer Code", "EAN", "Name", "Description", "Catalog Id"
                    # ] + override_columns + attr_mapping_columns

                    merged_df = merged_df[[col for col in cols_order if col in merged_df.columns]]

                    # Extraction du mapping cat√©gories Octopia <-> canal de vente
                    mapping_df = extract_octopia_channel_mapping(client, catalog_id, merged_df["Category Code"].unique())

                    # Remplacement de la colonne "Category Code" par "Channel Full Category Path"
                    merged_df = merged_df.merge(mapping_df, on="Category Code", how="left")
                    merged_df.drop(columns=["Category Code"], inplace=True)
                    merged_df.rename(columns={"Category Code": "Channel Full Category Path"}, inplace=True)

                    cols = merged_df.columns.tolist()
                    cols.insert(0, cols.pop(cols.index("Channel Full Category Path")))
                    merged_df = merged_df[cols]

                    # Extraction et cr√©ation du dataframe attribute_df
                    channel_paths = merged_df["Channel Full Category Path"].unique().tolist()
                    attribute_df = extract_channel_attributes(client, catalog_id, channel_paths)

                    # Mise √† jour des session_state
                    st.session_state["product_df"] = product_df
                    st.session_state["merged_df"] = merged_df
                    st.session_state["attribute_df"] = attribute_df
                    st.session_state["override_columns"] = override_columns
                    st.session_state["attr_mapping_columns"] = attr_mapping_columns
                    st.session_state["eans_validated"] = True
                    clear_after("eans_validated")
            else:
                st.error("Merci de renseigner la cl√© API, le Channel Catalog ID et au moins un EAN.")
                st.session_state["eans_validated"] = False

    # --- √âtape 2 : S√©lection attributs
    if st.session_state.get("eans_validated", False) and "attribute_df" in st.session_state:
        # 1) Base d'attributs + normalisation
        attribute_df = st.session_state["attribute_df"].copy()
        attribute_df["Status"] = attribute_df["Status"].fillna("").str.strip().str.capitalize()
        attribute_df["Channel Full Category Path"] = attribute_df["Channel Full Category Path"].fillna("")

        # # üëâ Filtre pour exclure les attributs dont le nom contient "[REMOVED BY MKP]"
        # attribute_df = attribute_df[~attribute_df["Attribute Name"].str.contains(r"\[REMOVED BY MKP\]", case=False, na=False)]
    
        # # 2) Priorisation "cat√©gorie sp√©cifique" > "Cross Categories"
        # #    + √† statut √©gal, on garde le plus restrictif: Required < Recommended < Optional
        # status_rank = {"Required": 0, "Recommended": 1, "Optional": 2, "": 3}
        # attribute_df["__is_cross"] = (attribute_df["Channel Full Category Path"] == "Cross Categories").astype(int)
        # attribute_df["__status_rank"] = attribute_df["Status"].map(status_rank).fillna(3).astype(int)

        # attribute_df = (
        #     attribute_df
        #     .sort_values(["Channel Attribute Id", "__is_cross", "__status_rank"], ascending=[True, True, True])
        #     .drop_duplicates(subset=["Channel Attribute Id"], keep="first")
        #     .reset_index(drop=True)
        # )

        # --- Nettoyage des attributs inutiles et doublons ---
        
        # 1Ô∏è‚É£ Supprimer les attributs d√©pr√©ci√©s marqu√©s [REMOVED BY MKP]
        attribute_df = attribute_df[
            ~attribute_df["Attribute Name"].str.contains(r"\[REMOVED BY MKP\]", case=False, na=False)
        ].copy()


        # 2Ô∏è‚É£ Identifier et r√©soudre les doublons sur la paire (Attribute Name, Channel Attribute Id)
        if {"Attribute Name", "Channel Attribute Id"}.issubset(attribute_df.columns):
            # -- Normalisation pr√©ventive du statut (au cas o√π ce ne serait pas d√©j√† fait)
            attribute_df["Status"] = attribute_df["Status"].fillna("").astype(str).str.strip().str.capitalize()
        
            # Priorit√© des statuts (plus petit = plus restrictif)
            status_rank = {"Required": 0, "Recommended": 1, "Optional": 2, "": 3}
            attribute_df["__status_rank"] = attribute_df["Status"].map(status_rank).fillna(3).astype(int)
        
            # Flag : 0 = sp√©cifique (pr√©f√©r√©), 1 = Cross Categories (moins prioritaire)
            attribute_df["__is_cross"] = attribute_df["Channel Full Category Path"].fillna("").eq("Cross Categories").astype(int)
        
            # Tri : on veut la combinaison la plus restrictive ET sp√©cifique en t√™te pour CHAQUE paire
            attribute_df = (
                attribute_df
                .sort_values(
                    by=["Attribute Name", "Channel Attribute Id", "__status_rank", "__is_cross"],
                    ascending=[True, True, True, True]
                )
                .drop_duplicates(subset=["Attribute Name", "Channel Attribute Id"], keep="first")
                .reset_index(drop=True)
            )
        
            # Nettoyage des colonnes techniques
            attribute_df.drop(columns=["__status_rank", "__is_cross"], inplace=True, errors="ignore")

        
        # # 2Ô∏è‚É£ Identifier les doublons sur le couple Attribute Name + Channel Attribute Id
        # if {"Attribute Name", "Channel Attribute Id"}.issubset(attribute_df.columns):
        #     # Ajout d‚Äôun flag pour prioriser les attributs sp√©cifiques √† la cat√©gorie
        #     attribute_df["__is_cross"] = attribute_df["Channel Full Category Path"].eq("Cross Categories").astype(int)
        
        #     attribute_df = (
        #         attribute_df.sort_values(
        #             by=["Attribute Name", "Channel Attribute Id", "__is_cross"],
        #             ascending=[True, True, True]  # Cross Categories (1) passe apr√®s la cat√©gorie sp√©cifique (0)
        #         )
        #         # Supprime les doublons, garde la version sp√©cifique
        #         .drop_duplicates(subset=["Attribute Name", "Channel Attribute Id"], keep="first")
        #         .reset_index(drop=True)
        #     )
        
        #     # Nettoyage du flag technique
        #     attribute_df.drop(columns=["__is_cross"], inplace=True, errors="ignore")

        # Label d'affichage
        attribute_df["display_label"] = attribute_df["Attribute Name"] + " [" + attribute_df["Status"].fillna("") + "]"

        with st.container(border=True):
            st.subheader("\u2777 Choix des attributs √† √©diter")

            # 3) S√©lection via pills (statuts)
            pills_choice = st.pills(
                "*S√©lectionner les attributs selon leur statut*",
                options=["Required", "Recommended", "Optional"],
                selection_mode="multi",
                default=st.session_state.get("pills_choice", []),
                key="pills_choice"
            )
            pills_norm = [p.capitalize() for p in pills_choice]
            selected_ids_pills = set(
                attribute_df.loc[
                    attribute_df["Status"].isin(pills_norm),
                    "Channel Attribute Id"
                ]
            )

            # 4) Multiselect : ne proposer que ce qui n'est PAS d√©j√† choisi via pills
            #    Pour √©viter les collisions de labels identiques, on passe des tuples (id, label) en "option value"
            options = [
                (row["Channel Attribute Id"], row["display_label"])
                for _, row in attribute_df.iterrows()
                if row["Channel Attribute Id"] not in selected_ids_pills
            ]

            # Valeurs par d√©faut conserv√©es si encore pr√©sentes
            default_opts = [
                opt for opt in st.session_state.get("selected_attr_opts", [])
                if opt in options
            ]

            selected_attr_opts = st.multiselect(
                "*S√©lectionner un ou plusieurs attributs √† √©diter (en plus de la s√©lection par statut)*",
                options=options,
                default=default_opts,
                key="selected_attr_opts",
                format_func=lambda o: o[1]  # n'affiche que le label
            )

            selected_ids_hand = {opt[0] for opt in selected_attr_opts}

            # 5) S√©lection finale = pills ‚à™ manuel
            selected_ids = selected_ids_pills.union(selected_ids_hand)

            # Reconstruction de selected_df depuis le DF prioris√©/d√©dupliqu√©
            # selected_df = (
            #     attribute_df[attribute_df["Channel Attribute Id"].isin(selected_ids)]
            #     .drop_duplicates(subset=["Channel Attribute Id"])
            # )
            selected_df = (
                attribute_df[attribute_df["Channel Attribute Id"].isin(selected_ids)]
                .drop_duplicates(subset=["Channel Attribute Id", "Attribute Name"])
                .reset_index(drop=True)
            )
            st.session_state["selected_df"] = selected_df

            # Auto-masquage de l'√©tape 3 si plus rien n'est s√©lectionn√©
            if selected_df.empty and st.session_state.get("attrs_validated", False):
                st.session_state["attrs_validated"] = False

            st.caption(f"*{len(selected_ids)} attribut(s) s√©lectionn√©(s)*")
            st.dataframe(selected_df[["Attribute Name", "Status"]], hide_index=True)

            # Validation pour passer √† l'√©tape 3
            if st.button("\u2713 Valider la s√©lection d'attributs", key="validate_attrs"):
                if len(selected_df) > 0:
                    st.session_state["attrs_validated"] = True
                else:
                    st.session_state["attrs_validated"] = False
                    st.warning("Merci de s√©lectionner au moins un attribut.")

    # --- √âtape 3 : G√©n√©ration et affichage du template
    selected_df = st.session_state.get("selected_df")
    if st.session_state.get("attrs_validated", False) and selected_df is not None and not selected_df.empty:
        with st.container(border=True):
            st.subheader("\u2778 G√©n√©ration et export du template")
            with st.spinner("G√©n√©ration du template en cours..."):
                # R√©cup√©ration des session_states
                merged_df = st.session_state["merged_df"]
                selected_df = st.session_state["selected_df"]
                override_columns = st.session_state["override_columns"]
                attr_mapping_columns = st.session_state["attr_mapping_columns"]

                # Pipeline de g√©n√©ration du template
                fixed_columns = [
                    "Channel Full Category Path", "Product Id", "Offer Code", "EAN", "Name", "Description", "Catalog Id"
                ]

                # Ajout pour extraction des images
                # Ajouter les images pr√©sentes (dans l‚Äôordre)
                fixed_columns += [f"Image {i}" for i in range(1, 7) if f"Image {i}" in merged_df.columns]

                selected_attr_cols = [row["Channel Attribute Id"] for _, row in selected_df.iterrows()]
                attr_mapping_to_keep = [col for col in attr_mapping_columns if col in selected_attr_cols]
                attr_mapping_to_drop = [col for col in attr_mapping_columns if col not in selected_attr_cols]
                merged_df_ = merged_df.drop(columns=[col for col in attr_mapping_to_drop if col in merged_df.columns])

                final_cols = (
                    fixed_columns +
                    override_columns +
                    attr_mapping_to_keep +
                    [col for col in selected_attr_cols if col not in override_columns + attr_mapping_to_keep]
                )

                for col in final_cols:
                    if col not in merged_df_.columns:
                        merged_df_[col] = ""

                template_df = merged_df_[[col for col in final_cols if col in merged_df_.columns]]

                # --- Nettoyage des doublons entre Cross Categories et cat√©gorie sp√©cifique ---
                # Si un attribut appara√Æt plusieurs fois (m√™me Channel Attribute Id),
                # on garde la version qui n'appartient PAS √† "Cross Categories"
                if "Channel Origin Category Name" in selected_df.columns:
                    selected_df = (
                        selected_df.sort_values(
                            by=["Channel Attribute Id", "Channel Origin Category Name"],
                            key=lambda col: col.eq("Cross Categories"),  # True = CrossCat ‚Üí passe apr√®s
                            ascending=True
                        )
                        .drop_duplicates(subset=["Channel Attribute Id"], keep="first")
                        .reset_index(drop=True)
                    )
                
                # (optionnel) avertissement si encore des doublons
                dupes = selected_df[selected_df.duplicated(subset=["Channel Attribute Id"], keep=False)]
                if not dupes.empty:
                    st.warning(
                        f"‚ö†Ô∏è Certains attributs apparaissent encore en double : "
                        f"{', '.join(dupes['Attribute Name'].unique())}"
                    )
                    
                # Renommage des colonnes pour plus de clart√© lors du remplissage
                id_to_label = {
                    row["Channel Attribute Id"]: f"{row['Attribute Name']} | {row['Channel Attribute Id']}"
                    for _, row in selected_df.iterrows()
                }
                template_df = template_df.rename(columns=id_to_label)

                # Mise √† jour des session_states
                client = st.session_state["client"]

                # G√©n√©ration du DataFrame des listes de valeurs
                dropdown_df = build_dropdown_dataframe(client, catalog_id, selected_df)

                # G√©n√©ration du DataFrame des infos attributs
                datainfo_df = build_datainfo_dataframe(selected_df)

                # Mise √† jour des session_states
                st.session_state["template_df"] = template_df
                st.session_state["dropdown_df"] = dropdown_df
                st.session_state["datainfo_df"] = datainfo_df
                st.session_state["template_generated"] = True

            # --- V√©rifie et corrige les doublons de colonnes avant affichage ---
            if template_df.columns.duplicated().any():
                seen = {}
                new_cols = []
                for col in template_df.columns:
                    if col not in seen:
                        seen[col] = 1
                        new_cols.append(col)
                    else:
                        seen[col] += 1
                        new_cols.append(f"{col}_{seen[col]}")  # rend unique
                template_df.columns = new_cols
                st.warning(
                    "‚ö†Ô∏è Des colonnes en double ont √©t√© d√©tect√©es et renomm√©es automatiquement "
                    "(ex: 'Attribut | ID_2')."
                )
            
            # Affichage du template
            st.dataframe(template_df, use_container_width=True, hide_index=True)

            # Export du template au format Excel
            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmpfile:
                build_and_export_excel(
                    template_df,
                    datainfo_df,
                    dropdown_df,
                    output_file=tmpfile.name
                )
                tmpfile.seek(0)

                # === Calcul du nom de fichier personnalis√© ===
                today_str = datetime.now().strftime("%Y-%m-%d")
                store_name = st.session_state.get("store_name", "")
                if not store_name:
                    store_name_safe = "no_name"
                else:
                    store_name_safe = "".join(c for c in store_name if c.isalnum() or c in ("_", "-")).strip().lower()
                nb_products = len(template_df)
                filename = f"template_{store_name_safe}_{today_str} [{nb_products} products].xlsx"

                st.download_button(
                    label="T√©l√©charger le template Excel",
                    data=tmpfile.read(),
                    file_name=filename
                )

# ---------- TAB2 : R√âINT√âGRER LE TEMPLATE DANS BEEZUP (baseline live + overrides conserv√©s) ----------
with tab2:
    st.title("R√©int√©gration du template dans BeezUP")

    # Garde-fou : param√®tres obligatoires
    if not api_key or not catalog_id:
        st.info("Renseigne la **cl√© API** et le **Channel Catalog ID** dans la barre lat√©rale pour continuer.")
        st.stop()

    # --- Upload du fichier template rempli ---
    with st.container(border=True):
        st.subheader("\u2776 Import du template compl√©t√©")
        uploaded_template = st.file_uploader(
            "Template **rempli** (Excel)", type=["xlsx"], key="upload_filled_template_live"
        )

    # Helpers parsing / normalisation
    def extract_attr_id(col_name: str) -> str | None:
        """En-t√™te 'Label | AttributeId' -> AttributeId, sinon None (colonne fixe)."""
        if not isinstance(col_name, str):
            return None
        parts = [p.strip() for p in col_name.split("|", 1)]
        return parts[1] if len(parts) == 2 and parts[1] else None

    def normalize_cell_value(v) -> str:
        """'code | label' -> 'code'; sinon valeur stripp√©e; vide -> ''."""
        if v is None or (isinstance(v, float) and pd.isna(v)):
            return ""
        s = str(v).strip()
        if not s:
            return ""
        if "|" in s:
            return s.split("|", 1)[0].strip()
        return s

    # ---- Baseline live : on r√©cup√®re l'√©tat courant via l'API (sur base des EANs du template) ----
    def fetch_current_state_by_eans(client: BeezUPClient, catalog_id: str, eans: list[str]) -> tuple[dict, dict]:
        """
        Retourne deux dicts :
          overrides[(product_id, attribute_id)] = valeur override actuelle (normalis√©e)
          effective[(product_id, attribute_id)] = valeur effective actuelle (override si pr√©sent, sinon mapping) (normalis√©e)
        """
        from beezup.extractor import extract_products
        overrides, effective = {}, {}

        if not eans:
            return overrides, effective

        product_infos = extract_products(client, catalog_id, eans)
        for prod in product_infos:
            pid = str(prod.get("productId", "")).strip()
            if not pid:
                continue
            ov = prod.get("overrides", {}) or {}
            mp = prod.get("attributeMappingValue", {}) or {}

            # overrides
            for attr_id, obj in ov.items():
                val = normalize_cell_value(obj.get("override", "")) if isinstance(obj, dict) else ""
                if val:
                    overrides[(pid, attr_id)] = val
                    effective[(pid, attr_id)] = val

            # mapping si pas d‚Äôoverride
            for attr_id, obj in mp.items():
                if (pid, attr_id) in effective:
                    continue
                val = normalize_cell_value(obj.get("attributeMappingValue", "")) if isinstance(obj, dict) else ""
                if val:
                    effective[(pid, attr_id)] = val

        return overrides, effective

    def build_payloads_from_template_with_live_baseline(filled_df: pd.DataFrame) -> list[dict]:
        """
        Construit la liste des payloads √† envoyer, en s'assurant que :
          - TOUTES les cl√©s d√©j√† en override sont renvoy√©es (m√™me si identiques)
          - Les nouvelles valeurs du template √©crasent celles des overrides
          - Les attributs non overrid√©s ne sont envoy√©s que si diff√©rents du baseline effectif
        """
        # map colonnes -> attribute ids
        dynamic_map = {j: extract_attr_id(c) for j, c in enumerate(filled_df.columns) if extract_attr_id(c)}

        # garde-fous
        for req in ["Product Id", "Catalog Id"]:
            if req not in filled_df.columns:
                raise ValueError(f"Colonne obligatoire manquante : '{req}'")

        # baseline live √† partir des EANs
        eans = []
        if "EAN" in filled_df.columns:
            eans = [str(x).strip() for x in filled_df["EAN"].dropna().astype(str).tolist() if str(x).strip()]
            eans = list(dict.fromkeys(eans))  # unique

        client = BeezUPClient(api_key)
        overrides_live, effective_live = fetch_current_state_by_eans(client, catalog_id, eans) if eans else ({}, {})

        rows_out = []
        for _, row in filled_df.iterrows():
            product_id = str(row.get("Product Id", "")).strip()
            catalog_id_row = str(row.get("Catalog Id", "")).strip()
            ean_row = str(row.get("EAN", "")).strip() if "EAN" in filled_df.columns else ""
            if not product_id or not catalog_id_row:
                continue

            # 1) point de d√©part : TOUTES les cl√©s d√©j√† en override pour ce produit
            payload = {attr_id: val for (pid, attr_id), val in overrides_live.items() if pid == product_id}

            # 2) appliquer les valeurs du template
            for j, attr_id in dynamic_map.items():
                if not attr_id:
                    continue
                colname = filled_df.columns[j]
                norm_val = normalize_cell_value(row.get(colname, None))
                if not norm_val:
                    continue

                key = (product_id, attr_id)
                base_eff = effective_live.get(key, "")
                in_override = key in overrides_live
                before = overrides_live.get(key, base_eff)

                if in_override:
                    # d√©j√† overrid√© : on renvoie ce qui est dans le template (m√™me si identique)
                    payload[attr_id] = norm_val
                else:
                    # pas d‚Äôoverride existant : envoyer seulement si diff de l‚Äôeffectif
                    if norm_val != base_eff:
                        payload[attr_id] = norm_val

            rows_out.append({
                "EAN": ean_row,
                "Product Id": product_id,
                "Catalog Id": catalog_id_row,
                "payload": payload,
                "count": len(payload)
            })
        return rows_out

    # --- Analyse + envoi ---
    if uploaded_template is not None:
        with st.container(border=True):
            st.subheader("\u2777 Pr√©paration et envoi des mises √† jour")

            try:
                filled_df = pd.read_excel(uploaded_template)
            except Exception as e:
                st.error(f"Impossible de lire le fichier Excel : {e}")
                st.stop()

            with st.spinner("Analyse du template & r√©cup√©ration de l‚Äô√©tat courant‚Ä¶"):
                candidates = build_payloads_from_template_with_live_baseline(filled_df)

            total_rows = len(candidates)
            total_updates = sum(c["count"] for c in candidates)
            st.write(f"- **Produits d√©tect√©s** : {total_rows}")
            st.write(f"- **Attributs envoy√©s (total)** : {total_updates}")

            recap = pd.DataFrame([
                {"EAN": c["EAN"], "Attributs envoy√©s": c["count"]}
                for c in candidates
            ])
            st.dataframe(recap, hide_index=True, use_container_width=True)

            # Bouton d‚Äôenvoi
            if st.button("‚ë¢ Envoyer dans BeezUP", type="primary"):
                if not candidates or total_updates == 0:
                    st.warning("Aucune valeur √† envoyer.")
                    st.stop()

                client = BeezUPClient(api_key)
                status_rows = []
                progress = st.progress(0.0, text="Envoi en cours‚Ä¶")

                for i, c in enumerate(candidates, start=1):
                    product_id = c["Product Id"]
                    ean = c["EAN"]
                    catalog_id_row = c["Catalog Id"]
                    payload = c["payload"]

                    if not payload:
                        status_rows.append({
                            "EAN": ean,
                            "Product Id": product_id,
                            "Count": 0,
                            "Status": "‚Äî (aucun changement)"
                        })
                        progress.progress(i / len(candidates))
                        continue

                    route = f"/user/channelCatalogs/{catalog_id_row}/products/{product_id}/overrides"
                    try:
                        client.put(route, data=payload)
                        status_rows.append({
                            "EAN": ean,
                            "Product Id": product_id,
                            "Count": len(payload),
                            "Status": "OK"
                        })
                    except Exception as e:
                        status_rows.append({
                            "EAN": ean,
                            "Product Id": product_id,
                            "Count": len(payload),
                            "Status": f"Erreur: {e}"
                        })

                    progress.progress(i / len(candidates))

                st.success("Envoi termin√©.")
                st.dataframe(pd.DataFrame(status_rows), hide_index=True, use_container_width=True)

    else:
        with st.container(border=True):
            st.info(
                "Importe ton **template compl√©t√©** (XLSX). "
                "L‚Äôapplication r√©cup√®re l‚Äô√©tat courant pour chaque EAN, "
                "et renvoie **tous les √©ditions existantes** + les changements du template."
            )


# ---------- TAB3 : MAPPER LES ATTRIBUTS NON MAPP√âS ----------
with tab3:
    st.title("üß© Mapping automatique des attributs non mapp√©s")

    # --- V√©rification des param√®tres essentiels ---
    if not api_key or not catalog_id:
        st.info("Renseigne la **cl√© API** et le **Channel Catalog ID** dans la barre lat√©rale pour continuer.")
        st.stop()

    client = BeezUPClient(api_key)
    store_id, channel_id = get_store_and_channel_ids(client, catalog_id)

    with st.container(border=True):
        st.subheader("Liste des Channel Attribute Id √† mapper")
        st.markdown("Colle ci-dessous la liste des **Channel Attribute Id** (un par ligne) que tu veux associer √† un champ personnalis√© vide.")

        # Cl√© dynamique pour forcer le reset du text_area
        if "attr_text_key" not in st.session_state:
            st.session_state["attr_text_key"] = "attr_text_0"
        
        attr_text = st.text_area(
            "Channel Attribute Id (un par ligne)",
            placeholder="Exemple :\n11250c20-b2c7-4c2b-b046-feedb4c6e6db\n30aa2fa3-4a47-4eee-b8a0-d63f5ef42199",
            key=st.session_state["attr_text_key"]
        )
        
        attrs_to_map = [a.strip() for a in attr_text.splitlines() if a.strip()]

        if not attrs_to_map:
            st.info("üìù Renseigne au moins un Channel Attribute Id ci-dessus.")
            st.stop()

        st.write(f"**{len(attrs_to_map)} attribut(s)** √† mapper seront associ√©s au champ personnalis√© vide.")

        if st.button("Mapper les attributs", type="primary"):       
            with st.spinner("V√©rification / cr√©ation du champ personnalis√© vide..."):
                # 1Ô∏è‚É£ V√©rifie s‚Äôil existe d√©j√† un champ perso vide
                custom_columns = client.get_custom_columns(store_id) or {}
                custom_id = None
                for col in custom_columns.get("customColumns", []):
                    if col.get("userColumName") == "Champ perso vide g√©n√©r√© par API":
                        custom_id = col.get("id")
                        break
        
                # Sinon, cr√©ation automatique
                if not custom_id:
                    custom_id = client.create_custom_column(store_id)
                    if not custom_id:
                        st.error("‚ùå Impossible de cr√©er le champ personnalis√© vide.")
                        st.stop()

            with st.spinner("Pr√©paration du mapping et envoi √† BeezUP..."):
                # 2Ô∏è‚É£ R√©cup√®re le mapping actuel
                mapping_data = client.get_channel_catalog_data(catalog_id) or {}
                existing = mapping_data.get("columnMappings", [])

                # 3Ô∏è‚É£ Pr√©pare le nouveau mapping complet
                new_payload = existing + [
                    {"channelColumnId": attr_id, "catalogColumnId": custom_id}
                    for attr_id in attrs_to_map
                ]

                # 4Ô∏è‚É£ Envoie la mise √† jour
                resp = client.update_column_mapping(catalog_id, new_payload)
                if resp.status_code in (200, 204):
                    st.success(f"‚úÖ {len(attrs_to_map)} attribut(s) ont √©t√© associ√©s au champ personnalis√© vide.")
                else:
                    st.error(f"‚ùå Erreur API ({resp.status_code}) : {resp.text}")


    





















